{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Architecture 2 (pipeline_0) - .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwTmhAkDQI2t"
      },
      "source": [
        "!pip install fastFM\n",
        "!pip install surprise \n",
        "!pip install lightfm\n",
        "!pip install hyperopt\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15\n",
        "!pip install annoy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9s9qGp5QXgD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3WbiHkCQZCm"
      },
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from numpy.linalg import inv\n",
        "from numpy.linalg import multi_dot\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import warnings\n",
        "from math import sqrt\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from fastFM import als\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from lightfm.datasets import fetch_movielens\n",
        "from lightfm import LightFM\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from scipy import sparse\n",
        "from lightfm import LightFM\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtSCP1GoQaPd"
      },
      "source": [
        "from hyperopt import tpe, fmin, hp, Trials, STATUS_OK,space_eval\n",
        "import sys\n",
        "import seaborn as sns\n",
        "from timeit import Timer\n",
        "from datetime import datetime\n",
        "\n",
        "from hyperopt import tpe, fmin, hp, Trials, STATUS_OK,space_eval\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from numpy.linalg import inv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy.linalg import inv\n",
        "from numpy.linalg import multi_dot\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from annoy import AnnoyIndex\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECDQDqUxYCLb"
      },
      "source": [
        "from ncf_singlenode import *\n",
        "from dataset import Dataset as NCFDataset\n",
        "from constants import SEED as DEFAULT_SEED\n",
        "SEED = DEFAULT_SEED"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_cAR3SZQmW2"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN2PwCSEQnvE"
      },
      "source": [
        "def get_data():\n",
        "  !pip install -U -q PyDrive\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  id = \"1FFYnZRIuzQLeBkDUuJpK_oRtUmdxMd9O\"\n",
        "\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile('final_dataset.csv')  \n",
        "  data = pd.read_csv('final_dataset.csv')\n",
        "  return data\n",
        "\n",
        "data = get_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52k6aNslQpnV"
      },
      "source": [
        "def train_test_split(data):\n",
        "\n",
        "  user_freq=data.groupby(['userId']).size().reset_index(name='counts')\n",
        "  users_lt3=user_freq[user_freq['counts']<3][['userId']]\n",
        "  users_ge3=user_freq[user_freq['counts']>=3][['userId']]\n",
        "  train1=pd.merge(data, users_lt3, on=['userId'],how='inner')\n",
        "  data1=pd.merge(data, users_ge3, on=['userId'],how='inner')\n",
        "  data1.sort_values(['userId', 'timestamp'], ascending=[True, False],inplace=True)\n",
        "  test=data1.groupby('userId').sample(frac=.3, random_state=2) \n",
        "  test_idx=data1.index.isin(test.index.to_list())\n",
        "  train=train1.append(data1[~test_idx])\n",
        "  return train, test, user_freq\n",
        "\n",
        "train, test, user_freq = train_test_split(data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mu8vD4rVr-L"
      },
      "source": [
        "## ANN-NCF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsMxRMBcVtx-"
      },
      "source": [
        "def run_ann_ncf_pipeline(train, test, user_freq):\n",
        "  train_df = train\n",
        "  test_df = test\n",
        "\n",
        "  def create_SVD_UV(train,test,user_freq):\n",
        "    train=train[['userId', 'movieId', 'rating']]\n",
        "    test=test[['userId', 'movieId', 'rating']]\n",
        "    reader = Reader(rating_scale=(1,5))\n",
        "    train = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader=reader)\n",
        "    test = Dataset.load_from_df(test[['userId', 'movieId', 'rating']], reader=reader)\n",
        "    raw_ratings = test.raw_ratings\n",
        "    threshold = int(1 * len(raw_ratings))\n",
        "    A_raw_ratings = raw_ratings[:threshold]\n",
        "    test = test.construct_testset(A_raw_ratings)\n",
        "\n",
        "    raw_ratings1 = train.raw_ratings\n",
        "    threshold = int(1 * len(raw_ratings1))\n",
        "    B_raw_ratings = raw_ratings1[:threshold]\n",
        "    train_test = train.construct_testset(B_raw_ratings)\n",
        "\n",
        "    model = SVD(n_epochs=50,n_factors=15,reg_all=0.1,lr_all=0.02)\n",
        "    trainset = train.build_full_trainset()\n",
        "    model.fit(trainset)\n",
        "\n",
        "    # Retrieiving inner ids, as used by surprise package during model training\n",
        "    user_inner_ids = [x for x in trainset.all_users()]\n",
        "    item_inner_ids = [i for i in trainset.all_items()]\n",
        "\n",
        "    # All ids mapped back to values in the actual train set \n",
        "    user_raw_ids = [trainset.to_raw_uid(x) for x in user_inner_ids]\n",
        "    item_raw_ids = [trainset.to_raw_iid(x) for x in item_inner_ids]\n",
        "\n",
        "    U = model.pu \n",
        "    V = model.qi\n",
        "\n",
        "    return U, V, trainset, user_inner_ids\n",
        "\n",
        "\n",
        "\n",
        "  U,V, trainset, user_inner_ids = create_SVD_UV(train,test,user_freq)\n",
        "\n",
        "  def run_ann(user_inner_ids, V, train_df):\n",
        "    #To be used for serialized process; user item search\n",
        "\n",
        "    f = 15 # n_factors\n",
        "    t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "    for i,v in zip(user_inner_ids,V):\n",
        "        t.add_item(i, v)\n",
        "\n",
        "    t.build(100)\n",
        "    t.save('test_user_item.ann')\n",
        "\n",
        "\n",
        "    u = AnnoyIndex(f, 'angular')\n",
        "    u.load('test_user_item.ann') \n",
        "    # super fast, will just map the file\n",
        "\n",
        "    user_ids_nn = list(user_freq[user_freq['num_movies']>50]['userId'].values)\n",
        "\n",
        "    def find_nn_greater_than_k(user_id, k=100):\n",
        "      return t.get_nns_by_vector(U[trainset.to_inner_uid(user_id)], k) \n",
        "\n",
        "    user_nn_items = list(map(find_nn_greater_than_k,user_ids_nn))\n",
        "    user_nn_items_dict={}\n",
        "    for userid,items in zip(user_ids_nn,user_nn_items):\n",
        "      user_nn_items_dict[userid] = [trainset.to_raw_iid(i) for i in items]\n",
        "    \n",
        "    user_nn_items_df = pd.concat({k: pd.Series(v) for k, v in user_nn_items_dict.items()})\n",
        "    user_nn_items_df = user_nn_items_df.reset_index()\n",
        "    user_nn_items_df = user_nn_items_df.drop(columns=['level_1'])\n",
        "    user_nn_items_df.rename(columns={'level_0': 'userId', 0: 'movieId'}, inplace=True)\n",
        "\n",
        "    merged_df= pd.merge(train_df,user_nn_items_df,how='inner',on=['userId','movieId'])\n",
        "\n",
        "    #concatenate data for users who rated less than 50 movies merged_df\n",
        "    user_ids_without_nn = set(list(train_df['userId'].unique())) - set(user_ids_nn)\n",
        "    user_ids_without_nn_df = pd.DataFrame(user_ids_without_nn,columns =['userId']) \n",
        "\n",
        "    user_ids_without_nn_df = pd.merge(train_df,user_ids_without_nn_df,how='inner',on='userId')\n",
        "\n",
        "    # ANN reduced matrix\n",
        "    train_concat_df = pd.concat([user_ids_without_nn_df, merged_df])\n",
        "    #train_concat_df = merged_df\n",
        "    return train_concat_df\n",
        "\n",
        "\n",
        "  train_concat_df = def run_ann(user_inner_ids, V, train_df)\n",
        "\n",
        "  def run_ncf(train_concat_df,test_df):\n",
        "    train=train_df[['userId' ,'movieId' ,'rating']]\n",
        "    test=test_df[['userId' ,'movieId' ,'rating']]\n",
        "    train.rename({'userId': 'userID', 'movieId': 'itemID'}, axis=1, inplace=True)\n",
        "    test.rename({'userId': 'userID', 'movieId': 'itemID'}, axis=1, inplace=True)\n",
        "    test=pd.merge(test,pd.DataFrame(np.unique(train['userID']),columns=['userID']),on=['userID'],how='inner')\n",
        "    data = NCFDataset(train=train, test=test, seed=SEED)\n",
        "    \n",
        "    EPOCHS = 100\n",
        "    BATCH_SIZE = 256\n",
        "\n",
        "    model = NCF (\n",
        "        n_users=data.n_users, \n",
        "        n_items=data.n_items,\n",
        "        model_type=\"NeuMF\",\n",
        "        n_factors=4,\n",
        "        layer_sizes=[16,8,4],\n",
        "        n_epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=1e-3,\n",
        "        verbose=10,\n",
        "        seed=SEED)\n",
        "    model.fit(data)\n",
        "\n",
        "    users, items, preds = [], [], []\n",
        "    item = list(train.itemID.unique())\n",
        "    for user in train.userID.unique():\n",
        "        user = [user] * len(item) \n",
        "        users.extend(user)\n",
        "        items.extend(item)\n",
        "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
        "\n",
        "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
        "\n",
        "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
        "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "  ann_ncf_predictions = run_ncf(train_concat_df, test_df)\n",
        "  return ann_ncf_predictions\n",
        "\n",
        "\n",
        "pipeline_predictions = run_ann_ncf_pipeline(train, test, user_freq)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}